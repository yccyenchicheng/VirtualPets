
<!DOCTYPE html>
<html>

<head>
    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-XB3PR2Y1TQ"></script>
    <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-XB3PR2Y1TQ');
    </script>

    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, shrink-to-fit=no">
    <title>Virtual Pets: Animatable Animal Generation in 3D Scenes </title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/4.5.0/css/bootstrap.min.css">
    <link href='https://fonts.googleapis.com/css?family=Source+Sans+Pro:300,400,500,600' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" href="./assets/css/Highlight-Clean.css">
    <link rel="stylesheet" href="./assets/css/styles.css">

    <!-- <link rel="apple-touch-icon" sizes="180x180" href="./assets/apple-touch-icon.png"> -->
    <!-- <link rel="icon" type="image/png" sizes="32x32" href="./favicon-32x32.png"> -->
    <!-- <link rel="icon" type="image/png" sizes="16x16" href="./favicon-16x16.png"> -->
    <link rel="apple-touch-icon" sizes="180x180" href="./assets/table-crop.png">
    <link rel="icon" type="image/png" sizes="32x32" href="./assets/table-crop.png">
    <link rel="manifest" href="./site.webmanifest">

    <meta property="og:site_name" content="Virtual Pets: Animatable Animal Generation in 3D Scenes " />
    <meta property="og:type" content="video.other" />
    <meta property="og:title" content="Virtual Pets: Animatable Animal Generation in 3D Scenes " />
    <meta property="og:description" content="Virtual Pets: Animatable Animal Generation in 3D Scenes , 2023." />
    <meta property="og:url" content="https://VirtualPets.github.io/" />
    <meta property="og:image" content="https://VirtualPets.github.io/assets/images/dreamfusion_samples.png" />

    <meta property="article:publisher" content="https://VirtualPets.github.io/" />
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="Virtual Pets: Animatable Animal Generation in 3D Scenes " />
    <meta name="twitter:description" content="Learning 4D animation of cats from 2D monocular videos." />
    <meta name="twitter:url" content="https://VirtualPets.github.io/" />
    <meta name="twitter:image" content="https://dreamfusion3d.github.io/assets/images/dreamfusion_samples.png" />
    <!-- <meta name="twitter:site" content="" /> -->

    <script src="./assets/js/video_comparison.js"></script>
    <script type="module" src="https://unpkg.com/@google/model-viewer@2.0.1/dist/model-viewer.min.js"></script>
</head>

<body>
    <!-- <div class="banner">
      <video class="video lazy"
          poster="https://dreamfusion-cdn.ajayj.com/sept28/banner_1x6_customhue_A.jpg"
          autoplay loop playsinline muted>
        <source data-src="https://dreamfusion-cdn.ajayj.com/sept28/banner_1x6_customhue_A.mp4" type="video/mp4"></source>
      </video>
    </div> -->

    <div class="highlight-clean" style="padding-bottom: 10px;">
        <!-- <div class="container" style="max-width: 768px;"> -->
        <div class="container" style="max-width: 768px;">
            <h1 class="text-center"><b>Virtual Pets</b>: Animatable Animal Generation in 3D Scenes</h1>
        </div>

        <div class="container" style="max-width: 800px; position: relative; left: 180px;">
            <div class="row authors">
                <div class="col-sm-3">
                    <h5 class="text-center"><a class="text-center" href="https://yccyenchicheng.github.io/">Yen-Chi Cheng</a></h5>
                    <h6 class="text-center">UIUC & Snap Research</h6>
                </div>
                <div class="col-sm-3">
                    <h5 class="text-center"><a class="text-center" href="https://hubert0527.github.io/">Chieh Hubert Lin</a></h5>
                    <h6 class="text-center">UC Merced</h6>
                </div>           
            </div>
        </div>

        <div class="container" style="max-width: 800px; position: relative; left: 90px;">
            <div class="row authors">
                <div class="col-sm-3">
                    <h5 class="text-center"><a class="text-center" href="https://mightychaos.github.io/">Chaoyang Wang</a></h5>
                    <h6 class="text-center">Snap Research</h6>
                </div>     
                <div class="col-sm-3">
                    <h5 class="text-center"><a class="text-center" href="https://yashkant.github.io/">Yash Kant</a></h5>
                    <h6 class="text-center">UofT & Snap Research</h6>
                </div>
                <div class="col-sm-3">
                    <h5 class="text-center"><a class="text-center" href="http://www.stulyakov.com/">Sergey Tulyakov</a></h5>
                    <h6 class="text-center">Snap Research</h6>
                </div>         
            </div>
        </div>

        <div class="container" style="max-width: 768px; position: relative; left: 90px;">
          <div class="row authors">
              <div class="col-sm-3">
                <h5 class="text-center"><a class="text-center" href="https://www.alexander-schwing.de/">Alex Schwing</a></h5>
                <h6 class="text-center">UIUC</h6>
              </div>
              <div class="col-sm-3">
                <h5 class="text-center"><a class="text-center" href="https://scholar.google.com/citations?user=3aE0r9QAAAAJ&hl=en">Liangyan Gui</a></h5>
                <h6 class="text-center">UIUC</h6>
              </div>
              <div class="col-sm-3">
                <h5 class="text-center"><a href="http://hsinyinglee.com/">Hsin-Ying Lee</a></h5>
                <h6 class="text-center">Snap Research</h6>
            </div>
          </div>
        </div>

        <div class="container" style="max-width: 768px;">
          <h4 class="text-center">Arxiv 2023</h4>
        </div>

        <div class="buttons" style="margin-bottom: 8px;">
            <a class="btn btn-light" role="button" href="https://arxiv.org/abs/2212.04493">
                <svg style="width:24px;height:24px;margin-left:-12px;margin-right:12px" viewBox="0 0 24 24">
                    <path fill="currentColor" d="M16 0H8C6.9 0 6 .9 6 2V18C6 19.1 6.9 20 8 20H20C21.1 20 22 19.1 22 18V6L16 0M20 18H8V2H15V7H20V18M4 4V22H20V24H4C2.9 24 2 23.1 2 22V4H4M10 10V12H18V10H10M10 14V16H15V14H10Z"></path>
                </svg>
                Paper
            </a>
            <a class="btn btn-light" role="button" href="https://github.com/yccyenchicheng/VirtualPets">
                <svg style="width:24px;height:24px;margin-left:-12px;margin-right:12px" viewBox="0 0 24 24">
                  <path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"/>
                </svg>
                Code
            </a>
        </div>
    </div>
    <!-- <hr class="divider" /> -->

    <div class="container" style="max-width: 768px;">
        <div class="row captioned_videos">
            <div class="col-md-12">
              <!-- <h2>VirtualPets - Overview</h2> -->
                <!-- Large format devices -->
                <!-- <video class="video lazy d-none d-xs-none d-sm-block" autoplay loop playsinline muted poster="https://dreamfusion-cdn.ajayj.com/sept28/wipe_opposite_6x4_smoothstep.jpg">
                    <source data-src="https://dreamfusion-cdn.ajayj.com/sept28/wipe_opposite_6x4_smoothstep.mp4" type="video/mp4"></source>
                </video> -->
                <!-- Small format devices -->
                <!-- <video class="video lazy d-xs-block d-sm-none" autoplay loop playsinline muted poster="https://dreamfusion-cdn.ajayj.com/sept28/shaded_3x3_smoothstep.jpg">
                    <source data-src="https://dreamfusion-cdn.ajayj.com/sept28/shaded_3x3_smoothstep.mp4" type="video/mp4"></source>
                </video> -->

                <!-- <video class="video lazy"
                  poster="./assets/1-teaser-v3-out-1.png"
                  autoplay loop playsinline muted>
                  <source data-src="./assets/teaser-vid.mp4" type="video/mp4"></source>
                </video> -->

                <!-- <video autoplay loop muted playsinline width="80%" style="display: block; margin: auto"> -->
                <!-- <video autoplay loop muted playsinline width="0%"> -->
                  <!-- <source src="./assets/teaser-vid-v1.mp4" type="video/mp4"> -->
                  <!-- <source src="./assets/teaser-vid-v2.mp4" type="video/mp4"> -->
                  <!-- <source src="./assets/1-teaser-v2.jpg" type="video/mp4"> -->
                <!-- </video> -->

                <img src="./assets/1-teaser-v2.jpg" style="max-width: 768px;">

                <!-- <p> <b>VirtualPets</b> is a diffusion-based 3D shape generator. It enables various applications.
                  (left) VirtualPets can generate 3D shapes conditioned on different input modalities, including partial shapes, images, and text. 
                  VirtualPets can even jointly handle multiple conditioning modalities while controlling the  strength for each of them.
                  (right) We showcase an application where we leverage pretrained 2D models to texture 3D shapes generated by VirtualPets.</p> -->
                <h6 class="caption" style="text-align: center;"> <font style="font-weight: bold;">Virtual Pets.&nbsp;</font> Given a 3D scene, we can generate diverse 3D animal motion sequences that are environment-aware.
                </h6>
            </div>
        </div>
    </div>

    <!-- <br> -->

    <hr class="divider" />

    <div class="container" style="max-width: 768px;">
      <div class="row">
          <div class="col-md-12">
              <h2>Abstract</h2>
              <p>
                  <!-- <strong> -->
                <font style="font-weight: bold;">
                Toward unlocking the potential of generative models in immersive 4D experiences, we introduce Virtual Pet, a novel pipeline to model realistic and diverse motions for target animal species within a 3D environment.
To circumvent the limited availability of 3D motion data aligned with environmental geometry, we leverage monocular internet videos and extract deformable NeRF representations for the foreground and static NeRF representations for the background. For this, we develop a reconstruction strategy, encompassing species-level shared template learning and per-video fine-tuning. 
Utilizing the reconstructed data, we then train a conditional 3D motion model to learn the trajectory and articulation of foreground animals in the context of 3D backgrounds.
We showcase the efficacy of our pipeline with comprehensive qualitative and quantitative evaluations. 
We also demonstrate versatility across unseen cats and indoor environments, producing temporally coherent 4D outputs for enriched virtual experiences.
                </font>
                  <!-- </strong> -->
              </p>
          </div>
      </div>
  </div>

    <!-- <hr class="divider" />

    <div class="container" style="max-width: 768px;">
      <div class="row">
        <div class="col-md-12">
            <h2>3D print</h2>
        </div>
      </div>

      <video autoplay loop muted playsinline width="20%">
        <source src="./assets/3d_print.mp4" type="video/mp4">
      </video>
    </div> -->

    <hr class="divider" />

    <div class="container" style="max-width: 768px;">
      <div class="row">
          <div class="col-md-12">
              <h2> Diverse Motion Generation </h2>
              <!-- TODO: change samples!!! -->
          </div>
      </div>

      <img src="assets/4-ours-diverse-v3.png" style="max-width: 768px;">

      <!-- <div class="container" style="max-width: 768px; position: relative; left: 0px;">
        <div class="row legends">
          <h6 class="text-left" style="position: relative; left: 25px;">Input</h6>
          <h6 class="text-left" style="position: relative; left: 60px;">Multimodal Shape Completion</h6>
          <h6 class="text-left" style="position: relative; left: 100px;">Input</h6>
          <h6 class="text-left" style="position: relative; left: 140px;">Multimodal Shape Completion</h6>
        </div>
      </div> -->

      <p> <font style="font-weight: bold;"> Diverse Environment-aware Motion Generation. </font> We show the diverse motion generations in different environments. 
        <font style="font-style: italic;"> (Top) </font> We show 4D generation given different starting poses G0.
        <font style="font-style: italic;"> (Bottom) </font> We show diverse motion outputs given the same starting pose in the same scene.
        The proposed method can generate diverse motions in different environments.
      </p>
    </div>

    <hr class="divider" />

    <div class="container" style="max-width: 768px;">
      <div class="row">
          <div class="col-md-12">
              <h2>Diverse Motion Generation (Multi-view)</h2>
          </div>
      </div>

      <img src="assets/a3-more-results-out.png" style="max-width: 768px;">

      <p> <font style="font-weight: bold;"> Diverse Environment-aware Motion Generation. </font> We show the diverse motion generations rendering in multi-view.
  
      </p>

    </div>

    <hr class="divider" />

    <div class="container" style="max-width: 768px;">
      <div class="row">
          <div class="col-md-12">
              <h2>Diverse Textures for Foreground and Background Objects</h2>
          </div>
      </div>

      <img src="assets/4-diff-tex-v3-out.png" style="max-width: 768px;">

      <p> <font style="font-weight: bold;"> Diverse textures. </font> We adopt <a href="https://daveredrum.github.io/Text2Tex/">Text2Tex</a> and 
        <a href="https://daveredrum.github.io/SceneTex/">SceneTex</a> to perform diverse texturing to both foreground objects and background scenes.
      </p>

    <hr class="divider" />

    <div class="container" style="max-width: 768px;">
      <div class="row">
          <div class="col-md-12">
              <h2>Overview of Virtual Pets</h2>
          </div>
      </div>

      <img src="assets/2-overview-p2-v4-out.png" style="max-width: 768px;">

      <p> <font style="font-weight: bold;"> The proposed framework of Virtual Pets. </font> 
        <font style="font-style: italic;"> (Left) </font> To extract 3D shapes and motions from monocular videos: we first learn a Species
        Articulated Template Model with an <a href="https://github.com/gengshan-y/rac">articulated NeRF</a> using a collection of cat videos. We then perform Per-Video Fine-tuning.
        For each video, we further reconstruct the background with a static NeRF. The articulated NeRF trained in species-level stage is loaded
        and fine-tuned in this stage to make sure the motions, which are Trajectory and Articulation, respect the reconstructed background shape.
        <br
        <font style="font-style: italic;"> (Right) </font> After that, we train an environment-aware 3D motion generator with a Trajectory VAE and an Articulation VAE. It generates 3D motions
        based on vertices of the foreground limbs, distance from foreground to background, and pointclouds sampled from the background
      </p>
    </div>

    <hr class="divider" />

    <div class="container" style="max-width: 768px;">
      <div class="row">
          <div class="col-md-12">
              <h2>Inference</h2>
          </div>
      </div>

      <img src="assets/2-overview-p3-v3-out.png" style="max-width: 768px;">

      <p> <font style="font-weight: bold;"> Inferenc: Texturing and Rendering. </font> 
        At the inference time, given textureless foreground and background meshes, we first adopt <a href="https://daveredrum.github.io/Text2Tex/">Text2Tex</a> and 
        <a href="https://daveredrum.github.io/SceneTex/">SceneTex</a> to texture the meshes. Meanwhile, we generate the motion sequence using the trained trajectory VAE 
        and articulation VAE. We then obtain the final predicted foreground mesh after deformation and transformation. 
        Finally, the 3D motion sequences and the 3D scene are rendered to videos given camera poses.
      </p>
    </div>

    <hr class="divider" />
    
    <!-- <div class="container" style="max-width: 768px;">
        <div class="row">
            <div class="col-md-12">
                <h2>Citation</h2>
                <code>
                    @inproceedings{cheng2023VirtualPets,<br>
                      &nbsp; title  = {{V}irtual {P}ets: nimatable Animal Generation in 3D Scenes},<br>
                      &nbsp; author={Cheng, Yen-Chi and Lee, Hsin-Ying and Tulyakov, Sergey and Schwing, Alexander G and Gui, Liang-Yan},<br>
                      &nbsp; booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},<br>
                      &nbsp; pages={4456--4465},<br>
                      &nbsp; year={2023},<br>
                }</code>
            </div>
        </div>
    </div> -->
    <div class="container" style="max-width: 768px;">
      <div class="row">
          <div class="col-md-12">
              <h2>Citation</h2>
              <code>
                  @inproceedings{cheng2023VirtualPets,<br>
                    &nbsp; title  = {{V}irtual {P}ets: Animatable Animal Generation in 3D Scenes},<br>
                    &nbsp; author={Cheng, Yen-Chi and Lin, Chieh Hubert and Wang, Chaoyang and Kant, Yash and Tulyakov, Sergey and Schwing, Alexander G and Gui, Liangyan and Lee, Hsin-Ying},<br>
                    &nbsp; journal = {arXiv},<br>
                    &nbsp; year={2023},<br>
              }</code>
          </div>
      </div>
  </div>


<!--     <hr class="divider" />
    <div class="container" style="max-width: 768px;">
        <div class="row">
            <div class="col-md-12">
                <h2>Citation</h2>
                <code>
                    @article{cheng2022VirtualPets,<br>
                    &nbsp; author = {Cheng, Yen-Chi and Lee, Hsin-Ying and Tuyakov, Sergey and Schwing, Alex and Gui, Liangyan},<br>
                    &nbsp; title  = {{VirtualPets}: Multimodal 3D Shape Completion, Reconstruction, and Generation},<br>
                    &nbsp; journal = {arXiv},<br>
                    &nbsp; year   = {2022},<br>
                }</code>
            </div>
        </div>
    </div> -->

    <hr class="divider" />

    <div class="container" style="max-width: 768px;">
      <div class="row">
          <div class="col-md-12">
            <h2>Acknowledgement</h2>
          </div>
          <h6> This webpage is borrowed from <a href="https://dreamfusion3d.github.io/">DreamFusion</a>. Thanks for their beautiful website! </h6>
      </div>
    </div>

    <script src="https://polyfill.io/v3/polyfill.js?features=IntersectionObserver"></script>
    <script src="/assets/js/yall.js"></script>
    <script>
        yall(
            {
                observeChanges: true
            }
        );
    </script>
    <script src="/assets/js/scripts.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/4.5.0/js/bootstrap.bundle.min.js"></script>
    <script src="https://uploads-ssl.webflow.com/51e0d73d83d06baa7a00000f/js/webflow.fd002feec.js"></script>
    <!-- Import the component -->
</body>

</html>